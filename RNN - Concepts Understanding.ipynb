{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4261c7ca",
   "metadata": {},
   "source": [
    "## Why Not Feedforward Networks?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdb81b4e",
   "metadata": {},
   "source": [
    "With a feed-forward network the new output at time ‘t+1’ has no relation with outputs at either time t, t-1 or t-2.\n",
    "\n",
    "So, feed-forward networks cannot be used when predicting a word in a sentence as it will have no absolute relation with the previous set of words."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c68b7fe",
   "metadata": {},
   "source": [
    "<img src=\"feed_forward.png\" width=\"800\" height=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "789cfbe3",
   "metadata": {},
   "source": [
    "### Issues using Feed Forward Neural Network(ANN) for sequence problems.\n",
    "\n",
    "###### 1) Variable size of input/output Neurons\n",
    "\n",
    "###### 2) Too much Computation\n",
    "\n",
    "###### 3) No parameter shareing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57a37703",
   "metadata": {},
   "source": [
    "But, with Recurrent Neural Networks, this challenge can be overcome.\n",
    "\n",
    "<img src=\"rnn_image.png\" width=\"800\" height=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a84b553c",
   "metadata": {},
   "source": [
    "In the above diagram, we have certain inputs at ‘t-1’ which is fed into the network. These inputs will lead to corresponding outputs at time ‘t-1’ as well.\n",
    "\n",
    "At the next timestamp, information from the previous input ‘t-1’ is provided along with the input at ‘t’ to eventually provide the output at ‘t’ as well.\n",
    "\n",
    "This process repeats, to ensure that the latest inputs are aware and can use the information from the previous timestamp is obtained."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe12a091",
   "metadata": {},
   "source": [
    "### What Are Recurrent Neural Networks?\n",
    "\n",
    "Recurrent Networks are a type of artificial neural network designed to recognize patterns in sequences of data, such as text, genomes, handwriting, the spoken word, numerical times series data emanating from sensors, stock markets and government agencies."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14fa55da",
   "metadata": {},
   "source": [
    "Recurrent Neural Network is a generalization of feedforward neural network that has an internal memory. RNN is recurrent in nature as it performs the same function for every input of data while the output of the current input depends on the past one computation. After producing the output, it is copied and sent back into the recurrent network. For making a decision, it considers the current input and the output that it has learned from the previous input."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d647da1",
   "metadata": {},
   "source": [
    "Unlike feedforward neural networks, RNNs can use their internal state (memory) to process sequences of inputs. This makes them applicable to tasks such as unsegmented, connected handwriting recognition or speech recognition. In other neural networks, all the inputs are independent of each other. But in RNN, all the inputs are related to each other."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e349ebb8",
   "metadata": {},
   "source": [
    "<img src=\"rnn_network.png\" width=\"800\" height=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95a60b9a",
   "metadata": {},
   "source": [
    "First, it takes the X(0) from the sequence of input and then it outputs h(0) which together with X(1) is the input for the next step. So, the h(0) and X(1) is the input for the next step. Similarly, h(1) from the next is the input with X(2) for the next step and so on. This way, it keeps remembering the context while training.\n",
    "The formula for the current state is\n",
    "\n",
    "<img src=\"rnn_formula.PNG\" width=\"800\" height=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b55f0640",
   "metadata": {},
   "source": [
    "W is weight, h is the single hidden vector, Whh is the weight at previous hidden state, Whx is the weight at current input state, tanh is the Activation funtion, that implements a Non-linearity that squashes the activations to the range[-1.1]\n",
    "Output:\n",
    "\n",
    "<img src=\"rnn_output.PNG\" width=\"500\" height=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52aee86f",
   "metadata": {},
   "source": [
    "### Advantages of Recurrent Neural Network\n",
    "    \n",
    "    1) RNN can model sequence of data so that each sample can be assumed to be dependent on previous ones\n",
    "    2) Recurrent neural network are even used with convolutional layers to extend the effective pixel neighbourhood.\n",
    "\n",
    "\n",
    "### Disadvantages of Recurrent Neural Network\n",
    "\n",
    "    1) Gradient vanishing and exploding problems.\n",
    "    2) Training an RNN is a very difficult task.\n",
    "    3) It cannot process very long sequences if using tanh or relu as an activation function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df2926e2",
   "metadata": {},
   "source": [
    "### What is Long Short Term Memory (LSTM)?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c787be0c",
   "metadata": {},
   "source": [
    "Long Short-Term Memory (LSTM) networks are a modified version of recurrent neural networks, which makes it easier to remember past data in memory. The vanishing gradient problem of RNN is resolved here. LSTM is well-suited to classify, process and predict time series given time lags of unknown duration. It trains the model by using back-propagation. In an LSTM network, three gates are present:\n",
    "\n",
    "<img src=\"LSTM.jpeg\" width=\"700\" height=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae80ba2a",
   "metadata": {},
   "source": [
    "    1) Input gate — discover which value from input should be used to modify the memory. Sigmoid function decides which values to let through 0,1. and tanh function gives weightage to the values which are passed deciding their level of importance ranging from-1 to 1.\n",
    "    \n",
    "\n",
    "<img src=\"lstm_form.PNG\" width=\"700\" height=\"700\">\n",
    "    \n",
    "    2) Forget gate — discover what details to be discarded from the block. It is decided by the sigmoid function. it looks at the previous state(ht-1) and the content input(Xt) and outputs a number between 0(omit this)and 1(keep this)for each number in the cell state Ct−1.\n",
    "\n",
    "<img src=\"lstm_forget.PNG\" width=\"700\" height=\"700\">\n",
    "    \n",
    "    3) Output gate — the input and the memory of the block is used to decide the output. Sigmoid function decides which values to let through 0,1. and tanh function gives weightage to the values which are passed deciding their level of importance ranging from-1 to 1 and multiplied with output of Sigmoid.\n",
    "\n",
    "<img src=\"Lstm_output.PNG\" width=\"700\" height=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10507db6",
   "metadata": {},
   "source": [
    "### BI-LSTM(Bi-directional long short term memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "913f0905",
   "metadata": {},
   "source": [
    "Bidirectional long-short term memory(bi-lstm) is the process of making any neural network o have the sequence information in both directions backwards (future to past) or forward(past to future). \n",
    "\n",
    "In bidirectional, our input flows in two directions, making a bi-lstm different from the regular LSTM. With the regular LSTM, we can make input flow in one direction, either backwards or forward. However, in bi-directional, we can make the input flow in both directions to preserve the future and the past information. For a better explanation, let’s have an example.    \n",
    "\n",
    "In the sentence “boys go to …..” we can not fill the blank space. Still, when we have a future sentence “boys come out of school”, we can easily predict the past blank space the similar thing we want to perform by our model and bidirectional LSTM allows the neural network to perform this."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e69ebe6",
   "metadata": {},
   "source": [
    "<img src=\"Bidirectional_lstm.png\" width=\"700\" height=\"700\">\n",
    "\n",
    "                                              Bidirectional LSTM Architecture"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
